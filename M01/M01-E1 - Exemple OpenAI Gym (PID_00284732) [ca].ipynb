{"cells":[{"cell_type":"markdown","metadata":{"id":"UQAujm6hlf-1"},"source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprenentatge per reforç</p>\n","<p style=\"margin: 0; text-align:right;\">Màster universitari de Ciència de Dades</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>\n","\n","\n","# Mòdul 1: exemples d'OpenAI Gym\n","\n","En aquest _notebook_ carregarem alguns dels escenaris d'OpenAI Gym i veurem la interacció entre alguns agents i aquests escenaris o entorns."]},{"cell_type":"markdown","metadata":{"id":"7l1X8jTzlf-5"},"source":["## 1. CartPole\n","En aquest primer exemple carregarem l'entorn CartPole i farem algunes proves."]},{"cell_type":"markdown","metadata":{"id":"g7qudfIolf-5"},"source":["### 1.1. Càrrega de dades\n","\n","El codi següent carrega els paquets necessaris per a l'exemple, crea l'entorn mitjançant el mètode `make` i imprimeix per pantalla la dimensió de l'espai d'accions (dues accions: 0 = esquerra i 1 = dreta), de l'espai d'observacions (quatre observacions: posició del carretó, velocitat del carretó, angle del pal i velocitat del pal en la punta) i el rang de la variable de recompensa (de menys infinit a més infinit)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJd6xODOlf-6"},"outputs":[],"source":["import gym\n","import numpy as np\n","\n","env = gym.make('CartPole-v1')\n","print(\"Action space is {} \".format(env.action_space))\n","print(\"Observation space is {} \".format(env.observation_space))\n","print(\"Reward range is {} \".format(env.reward_range))"]},{"cell_type":"markdown","metadata":{"id":"2iL1i-Ozlf-8"},"source":["Seguidament, reinicialitzem l'entorn (acció que cal fer sempre després de la creació d'aquest entorn) i inicialitzem les variables que guardaran el nombre de passos executats (`t`), la recompensa acumulada (`total_reward`) i la variable que ens indicarà quan finalitza un episodi (`done`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTJpH74blf-8"},"outputs":[],"source":["# Environment reset\n","obs = env.reset()\n","t, total_reward, done = 0, 0, False"]},{"cell_type":"markdown","metadata":{"id":"Jj0HXXDnlf-8"},"source":["### 1.2. Execució d'un episodi\n","\n","A continuació, farem l'execució d'un episodi de l'entorn CartPole utilitzant un agent que selecciona les accions de manera aleatòria.\n","\n","El codi següent fa l'execució d'un episodi de l'entorn (aquest finalitza quan la variable `done` pren el valor `True`). L'agent s'implementa mitjançant el mètode  `env.action_space.sample()`, que selecciona una acció a l'atzar. Per a cada pas (_time step_), s'imprimeixen per pantalla l'observació que genera l'entorn (els quatre valors esmentats anteriorment), l'acció seleccionada i la recompensa obtinguda en aquest pas (+1 en cada acció fins que finalitza l'episodi)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"bstLdPk-lf-9"},"outputs":[],"source":["while not done:\n","    \n","    # Render the environment (Doesn't work in Google Colab) \n","    #env.render() # --- Uncomment if you want to see the episode\n","    \n","    # Get random action (this is the implementation of the agent)\n","    action = env.action_space.sample()\n","    \n","    # Execute action and get response\n","    new_obs, reward, done, info = env.step(action)\n","    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n","    \n","    obs = new_obs\n","    total_reward += reward\n","    t += 1\n","    \n","total_reward += reward\n","t += 1\n","print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))"]},{"cell_type":"markdown","metadata":{"id":"908xEEDplf--"},"source":["Finalment, imprimim els resultats i tanquem l'entorn."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"M3riNJNulf-_"},"outputs":[],"source":["print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"SYvdGAOulf-_"},"source":["### 1.3. Simulació de diversos episodis\n","\n","El fragment de codi següent repeteix el procés de l'apartat anterior per al nombre d'episodis definit en la variable `num_episodes`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGkEO0f3lf_A"},"outputs":[],"source":["num_episodes = 10\n","\n","for episode in range(num_episodes):\n","\n","    # Environment reset\n","    obs = env.reset()\n","    t, total_reward, done= 0, 0, False\n","    \n","    print('Running episode {} '.format(episode+1))\n","    \n","    while not done:\n","    \n","        # Render the environment (Doesn't work in Google Colab)\n","        #env.render() # --- Uncomment if you want to see the episode\n","    \n","        # Get random action (this is the implementation of the agent)\n","        action = env.action_space.sample()\n","    \n","        # Execute action and get response\n","        new_obs, reward, done, info = env.step(action)\n","        print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n","    \n","        obs = new_obs\n","        total_reward += reward\n","        t += 1\n","        \n","    total_reward += reward\n","    t += 1\n","    print(\"Obs: {} -> Action: {} and reward: {}\".format(np.round(obs, 3), action, reward))\n","    print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, total_reward))\n","    print('')\n","    \n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"3w1IbWG8lf_A"},"source":["## 2. FrozenLake\n","En aquest segon exemple carregarem l'entorn FrozenLake i tornarem a fer algunes proves."]},{"cell_type":"markdown","metadata":{"id":"4kMun1fzlf_A"},"source":["### 2.1. Càrrega de dades\n","\n","De la mateixa forma que en l'exemple inicial, el codi següent carrega els paquets necessaris per a l'exemple, crea l'entorn mitjançant el mètode `make` i imprimeix per pantalla la dimensió de l'espai d'accions (0 = esquerra, 1 = dreta, 2 = a baix i 3 = a dalt), l'espai d'observacions (un número del 0 al 15 que indica la posició de l'agent en l'entorn) i el rang de la variable de recompensa (0 per a qualsevol acció excepte si s'arriba a la casella de destinació, i en aquest cas la recompensa és 1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGvyKBuDlf_B"},"outputs":[],"source":["import time\n","\n","env = gym.make('FrozenLake-v1')\n","print(\"Action space is {} \".format(env.action_space))\n","print(\"Observation space is {} \".format(env.observation_space))\n","print(\"Reward range is {} \".format(env.reward_range))"]},{"cell_type":"markdown","metadata":{"id":"4OfRintplf_B"},"source":["### 2.2. Execució d'un episodi\n","\n","A continuació, executarem un episodi de l'entorn FrozenLake utilitzant un agent que selecciona les accions de manera aleatòria.\n","\n","En el codi següent inicialitzem l'entorn, definim el màxim nombre de passos per a episodi (`max_steps`) i fem l'execució d'un episodi de l'entorn (aquest finalitza quan la variable 'done' pren el valor 'True' o quan s'aconsegueix el nombre màxim de passos estipulat). De nou, utilitzem un agent que implementa una política completament aleatòria (`env.action_space.sample()`). Mitjançant el mètode `env.render()`, podem anar veient l'evolució de l'agent en l'entorn des de la casella de sortida S fins que arriba a la casella de destinació G o cau en un forat H."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"OXVSeoIElf_B"},"outputs":[],"source":["# Environment reset\n","obs = env.reset()\n","t, total_reward, done = 0, 0, False\n","max_steps = 100\n","\n","# Render the environment (Doesn't work in Google Colab)\n","#env.render() # --- Uncomment if you want to see the episode\n","#print('') # --- Uncomment if you want to see the episode\n","#time.sleep(0.1) # --- Uncomment if you want to see the episode\n","\n","while t < max_steps:\n","    # Get random action (this is the implementation of the agent)\n","    action = env.action_space.sample()\n","    \n","    # Execute action and get response\n","    obs, reward, done, info = env.step(action)\n","    \n","    # Render the environment (Doesn't work in Google Colab)\n","    #env.render() # --- Uncomment if you want to see the episode\n","    #print('') # --- Uncomment if you want to see the episode\n","        \n","    t += 1\n","    if done:\n","        break\n","    time.sleep(0.1)\n","\n","print(\"Episode finished after {} timesteps and reward was {} \".format(t, reward))\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"bLEM3TvQlf_C"},"source":["### 2.3. Simulació de diversos episodis\n","\n","El fragment de codi següent repeteix el procés de l'apartat anterior per al nombre d'episodis definit en la variable `num_episodes`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6PAeUi7lf_C"},"outputs":[],"source":["num_episodes = 10\n","\n","for episode in range(num_episodes):\n","\n","    # Environment reset\n","    obs = env.reset()\n","    t, done = 0, False\n","    \n","    print('Running episode {} '.format(episode+1))\n","\n","    # Render the environment (Doesn't work in Google Colab)\n","    #env.render() # --- Uncomment if you want to see the episode\n","    #print('') # --- Uncomment if you want to see the episode\n","    #time.sleep(0.1) # --- Uncomment if you want to see the episode\n","    \n","    while t < max_steps:\n","        # Get random action (this is the implementation of the agent)\n","        action = env.action_space.sample()\n","    \n","        # Execute action and get response\n","        obs, reward, done, info = env.step(action)\n","        \n","        # Render the environment (Doesn't work in Google Colab)\n","        #env.render() # --- Uncomment if you want to see the episode\n","        #print('') # --- Uncomment if you want to see the episode\n","        \n","        t += 1\n","        if done:\n","            break\n","        time.sleep(0.1)\n","      \n","    print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, reward))\n","    print('')"]},{"cell_type":"markdown","metadata":{"id":"YIz3vUOjlf_C"},"source":["### 2.4. Càlcul de la recompensa total de diversos episodis\n","\n","Per mesurar l'eficiència de l'agent, podem calcular la recompensa total de diversos episodis. Atès que en cada episodi la recompensa acumulada és 0 si no s'arriba a la cel·la de destinació i 1 si s'aconsegueix l'objectiu, mesurar la recompensa total acumulada d'un nombre d'episodis ens dona una mesura del percentatge d'èxit del nostre agent."]},{"cell_type":"markdown","metadata":{"id":"gLS5u7yXlf_C"},"source":["El fragment de codi següent repeteix el procés de l'apartat anterior per al nombre d'episodis definit en la variable `num_episodes` i calcula el percentatge d'encert de l'agent. S'omet la renderització de l'entorn amb l'objectiu d'agilitar l'execució."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVrqDMnylf_D"},"outputs":[],"source":["num_episodes = 1000\n","total_reward = 0\n","\n","for episode in range(num_episodes):\n","\n","    # Environment reset\n","    obs = env.reset()\n","    t, done = 0, False\n","\n","    # Render the environment (Doesn't work in Google Colab)\n","    #env.render() --- Uncomment if you want to see the path of the agen  \n","\n","    while t < max_steps:\n","        # Get random action (this is the implementation of the agent)\n","        action = env.action_space.sample()\n","    \n","        # Execute action and get response\n","        obs, reward, done, info = env.step(action)\n","        \n","        # Render the environment (Doesn't work in Google Colab)\n","        #env.render() --- Uncomment if you want to see the path of the agent\n","        \n","        total_reward += reward\n","        t += 1\n","        if done:\n","            break\n","    \n","success_rate = total_reward*100/num_episodes\n","print(\"{} successes in {} episodes: {} % of success\".format(total_reward, num_episodes, success_rate))"]},{"cell_type":"markdown","metadata":{"id":"6ZK5hoODlf_D"},"source":["### 2.5. Entrenament d'un agent\n","\n","Tal com hem pogut veure en l'apartat anterior, com que l'agent utilitzat tria les accions a l'atzar, és gairebé impossible arribar a la casella de destinació G amb aquesta política (el percentatge d'èxit està en l'1 % o el 2 %). Entrenarem un agent utilitzant el mètode Q-Learning. Aquest mètode (que s'estudiarà en mòduls posteriors) es pot implementar mitjançant una taula que va actualitzant-se a partir de la interacció de l'agent amb l'entorn.\n","El codi següent implementa aquest mètode i fa l'entrenament de l'agent a partir de l'execució de diversos episodis.\n","\n","__Nota__: recordeu que les simulacions executades tenen un component aleatori i els percentatges poden variar d'una execució a una altra."]},{"cell_type":"markdown","metadata":{"id":"sB_42JoUlf_D"},"source":["Comencem important alguns paquets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EGvYEyLlf_D"},"outputs":[],"source":["import pickle"]},{"cell_type":"markdown","metadata":{"id":"YOQqBraZlf_E"},"source":["Inicialitzem algunes variables del mètode que volem implementar, entre les quals hi ha el nombre d'episodis (`num_episodes`) i el nombre màxim de passos per cada episodi (`max_steps`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfVS7jeRlf_E"},"outputs":[],"source":["epsilon = 0.9\n","num_episodes = 100000\n","max_steps = 100\n","\n","learning_rate = 0.81\n","gamma = 0.96"]},{"cell_type":"markdown","metadata":{"id":"uPGMCZhllf_E"},"source":["Inicialitzem a zero tots els valors de la taula de la funció Q (de setze estats per quatre accions cada estat), que acabarà donant-nos una idea de quina és la millor acció per a cada estat."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNf9Et_Rlf_E"},"outputs":[],"source":["Q = np.zeros((env.observation_space.n, env.action_space.n))\n","print(Q)"]},{"cell_type":"markdown","metadata":{"id":"XDeK8Hp2lf_E"},"source":["El codi següent defineix les funcions que caracteritzen l'agent (s'estudiaran en mòduls posteriors d'aquest curs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOfRoKxklf_F"},"outputs":[],"source":["def choose_action(state):\n","    action=0\n","    if np.random.uniform(0, 1) < epsilon:\n","        action = env.action_space.sample()\n","    else:\n","        action = np.argmax(Q[state, :])\n","    return action\n","\n","def learn(state, new_state, reward, action):\n","    predict = Q[state, action]\n","    target = reward + gamma * np.max(Q[new_state, :])\n","    Q[state, action] = Q[state, action] + learning_rate * (target - predict)"]},{"cell_type":"markdown","metadata":{"id":"W6YGwDCAlf_F"},"source":["El codi següent fa tantes partides del joc com s'indiquen en la variable `num_episodes`. En cada partida (episodi), l'agent va interactuant amb l'entorn i, com a fruit d'aquesta interacció, va actualitzant els valors de la taula _Q_. En el codi s'ha comentat el mètode `env.render()` amb l'objectiu de no saturar la pantalla. Així mateix, s'imprimeixen per pantalla els episodis en els quals l'agent aconsegueix la casella de destinació."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKiO8p8plf_F"},"outputs":[],"source":["# Start\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    t = 0\n","    \n","    while t < max_steps:\n","        # Render the environment (Doesn't work in Google Colab)\n","        #env.render() --- Uncomment if you want to see the path of the agent\n","        action = choose_action(state)  \n","        state2, reward, done, info = env.step(action)  \n","        learn(state, state2, reward, action)\n","        state = state2\n","        t += 1\n","       \n","        if done:\n","            break\n","\n","    if reward == 1:\n","        print(\"Episode {} finished after {} timesteps and reward was {} \".format(episode+1, t, reward)) "]},{"cell_type":"markdown","metadata":{"id":"3jxLwqyAlf_F"},"source":["Podem veure els valors finals de la taula _Q_ després de l'entrenament."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Yy9dkDslf_F"},"outputs":[],"source":["print(Q)"]},{"cell_type":"markdown","metadata":{"id":"ufjSS9NHlf_F"},"source":["### 2.6. Comprovació de la millora\n","En aquest últim apartat comprovarem que l'agent dissenyat aconsegueix millors prestacions que l'agent aleatori."]},{"cell_type":"markdown","metadata":{"id":"R3Xu7sbslf_G"},"source":["El codi és molt semblant al que hem utilitzat mentre entrenàvem l'agent, però s'omet la part d'aprenentatge d'aquest agent. Per a això, simularem diversos episodis utilitzant els valors de la taula _Q_ obtinguda en l'entrenament. Concretament, l'agent selecciona el valor màxim de la taula _Q_ per a cada estat:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrPS1Yaqlf_G"},"outputs":[],"source":["def choose_action_max(state):\n","    action = np.argmax(Q[state, :])\n","    return action"]},{"cell_type":"markdown","metadata":{"id":"uQhJXsOulf_G"},"source":["De nou, calculem la recompensa total de diversos episodis i calculem el percentatge d'encert, que, com es pot comprovar, és superior al de l'agent aleatori."]},{"cell_type":"markdown","metadata":{"id":"VoYMS7xtlf_G"},"source":["En el codi s'ofereix l'oportunitat de visualitzar (de manera diferent a la vista fins a aquest moment) els últims episodis de la simulació (indicats en la variable `num_shows`). Aquesta opció no està disponible a Google Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_ruuWHTlf_G"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","num_episodes = 1000\n","total_reward = 0\n","num_shows = 5\n","show_episode = False\n","\n","# start\n","for episode in range(num_episodes):\n","\n","    if (num_episodes - episode) <= num_shows:\n","        show_episode = False # Set to 'False' in Google Colab\n","        \n","    state = env.reset()\n","    \n","    if show_episode == True:\n","        print('')\n","        print('')\n","        print(\"*** Episode: \", episode+1)\n","        print('')\n","        print('')\n","        time.sleep(0.8)\n","        clear_output(wait=True)\n","        env.render()\n","    \n","    t = 0\n","    while t < 100:\n","        action = choose_action_max(state)  \n","        state, reward, done, info = env.step(action)  \n","        \n","        if show_episode == True:\n","            time.sleep(0.5)\n","            clear_output(wait=True)\n","            env.render()\n","        if done:\n","            break\n","\n","    if show_episode == True:\n","        time.sleep(0.8)\n","        clear_output(wait=True)\n","        print('')\n","        print('')\n","        print('Reward = {}'.format(reward))\n","        print('')\n","        print('')\n","        time.sleep(0.8)\n","        clear_output(wait=True)\n","    \n","    total_reward += reward\n","    \n","success_rate = total_reward*100/num_episodes\n","print(\"{} successes in {} episodes: {} % of success\".format(total_reward, num_episodes, success_rate))"]}],"metadata":{"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}