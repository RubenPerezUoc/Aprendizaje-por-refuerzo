{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprenentatge per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Màster universitari de Ciència de Dades</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mòdul 9: Exemple de DQN en l'entorn CartPole\n",
    "\n",
    "\n",
    "En aquest _notebook_ veurem un exemple d'implementació d'una Deep Q-Network (DQN) utilitzant un entorn ja predefinit en OpenAI.\n",
    "\n",
    "Tant per a aquest exemple com per a les pràctiques posteriors s'utilitzarà el <i>framework</i> de __Pytorch__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entorn CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest exemple usarem un entorn ja definit en la llibreria d'OpenAI, però cal tenir present que en altres problemes més concrets l'entorn necessitarà ser definit.\n",
    "\n",
    "CartPole consisteix a aprendre a controlar un objecte. El joc consta d'un carretó i d'un pal col·locat verticalment damunt del carretó. El pal s'aguanta únicament per la gravetat, mentre que el carretó es mou a dreta i esquerra sense parar. L'objectiu de l'agent és controlar la velocitat del carretó augmentant-la o disminuint-la amb la condició d'evitar que el pal caigui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Establiment de l'entorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lloc, carregarem la llibreria __gym__ i inicialitzarem l'entorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada entorn té definit tot el necessari perquè un agent pugui aprendre: tenim un joc que funciona d'una manera determinada i podem entrenar un agent perquè aprengui a jugar a aquest joc sense cap més ajuda que la d'experimentar-hi observant, actuant i rebent recompenses. Així, l'entorn del joc ja defineix quines accions es poden prendre, quines situacions poden presentar-se, en què consistirà la recompensa, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, podem visualitzar l'entorn de __CartPole__ generant un bucle sobre uns pocs episodis i, en acabar, el tanquem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02148017 -0.04427617 -0.04407625 -0.00787081]\n",
      "[-0.02236569 -0.2387392  -0.04423367  0.27058622]\n",
      "[-0.02714048 -0.43320294 -0.03882194  0.54899605]\n",
      "[-0.03580453 -0.23755776 -0.02784202  0.24433862]\n",
      "[-0.04055569 -0.04204943 -0.02295525 -0.05699472]\n",
      "[-0.04139668 -0.23683485 -0.02409514  0.22835818]\n",
      "[-0.04613337 -0.43160433 -0.01952798  0.51334431]\n",
      "[-0.05476546 -0.62644587 -0.00926109  0.79981005]\n",
      "[-0.06729438 -0.82143957  0.00673511  1.08956533]\n",
      "[-0.08372317 -0.62640705  0.02852642  0.79900329]\n",
      "[-0.09625131 -0.82190849  0.04450648  1.10052177]\n",
      "[-0.11268948 -1.01758697  0.06651692  1.40682942]\n",
      "[-0.13304122 -1.2134684   0.09465351  1.71954335]\n",
      "[-0.15731059 -1.40953893  0.12904437  2.04011924]\n",
      "[-0.18550137 -1.21595922  0.16984676  1.78999783]\n",
      "[-0.20982055 -1.4125311   0.20564671  2.13030807]\n",
      "Episode finished after 16 timesteps\n",
      "[-0.02262192 -0.02671578 -0.00931504  0.03742169]\n",
      "[-0.02315624 -0.22170292 -0.00856661  0.32715113]\n",
      "[-0.0275903  -0.41670187 -0.00202359  0.61712027]\n",
      "[-0.03592433 -0.22155171  0.01031882  0.32380069]\n",
      "[-0.04035537 -0.41681906  0.01679483  0.6197198 ]\n",
      "[-0.04869175 -0.22193565  0.02918923  0.33237337]\n",
      "[-0.05313046 -0.41746065  0.0358367   0.63411635]\n",
      "[-0.06147968 -0.22285643  0.04851902  0.35293118]\n",
      "[-0.0659368  -0.41863354  0.05557765  0.66051025]\n",
      "[-0.07430948 -0.22432724  0.06878785  0.38583224]\n",
      "[-0.07879602 -0.0302458   0.0765045   0.11560626]\n",
      "[-0.07940094 -0.22637584  0.07881662  0.43141117]\n",
      "[-0.08392845 -0.03245322  0.08744485  0.16457838]\n",
      "[-0.08457752  0.16131519  0.09073641 -0.09928767]\n",
      "[-0.08135121 -0.03498212  0.08875066  0.22058694]\n",
      "[-0.08205086 -0.23125318  0.0931624   0.53989328]\n",
      "[-0.08667592 -0.42755274  0.10396026  0.86041589]\n",
      "[-0.09522697 -0.23398859  0.12116858  0.60214652]\n",
      "[-0.09990675 -0.43057839  0.13321151  0.93040562]\n",
      "[-0.10851831 -0.62722228  0.15181963  1.26180664]\n",
      "[-0.12106276 -0.82392412  0.17705576  1.59792927]\n",
      "[-0.13754124 -1.02064726  0.20901434  1.94018363]\n",
      "Episode finished after 22 timesteps\n",
      "[-0.00706834 -0.04368595  0.0400685  -0.01444486]\n",
      "[-0.00794206  0.15083915  0.0397796  -0.29422132]\n",
      "[-0.00492528 -0.0448267   0.03389518  0.01073738]\n",
      "[-0.00582181 -0.24041794  0.03410993  0.31391901]\n",
      "[-0.01063017 -0.43600878  0.04038831  0.61716097]\n",
      "[-0.01935035 -0.24147363  0.05273153  0.33746715]\n",
      "[-0.02417982 -0.43730476  0.05948087  0.64630143]\n",
      "[-0.03292591 -0.63320286  0.0724069   0.95710599]\n",
      "[-0.04558997 -0.43912538  0.09154902  0.68802158]\n",
      "[-0.05437248 -0.6353907   0.10530945  1.00806456]\n",
      "[-0.06708029 -0.83174884  0.12547074  1.33187384]\n",
      "[-0.08371527 -1.02820971  0.15210822  1.66104021]\n",
      "[-0.10427946 -0.83515218  0.18532902  1.4193451 ]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.01283545 -0.0483146   0.04647922  0.00965031]\n",
      "[-0.01380174 -0.24407124  0.04667222  0.31662814]\n",
      "[-0.01868316 -0.04964407  0.05300479  0.03902161]\n",
      "[-0.01967605  0.1446793   0.05378522 -0.23647797]\n",
      "[-0.01678246 -0.05116817  0.04905566  0.07267366]\n",
      "[-0.01780582 -0.24695783  0.05050913  0.38042142]\n",
      "[-0.02274498 -0.44275927  0.05811756  0.6885924 ]\n",
      "[-0.03160017 -0.63863761  0.07188941  0.99899079]\n",
      "[-0.04437292 -0.44454645  0.09186923  0.72972331]\n",
      "[-0.05326385 -0.25080628  0.10646369  0.46731015]\n",
      "[-0.05827997 -0.05733688  0.11580989  0.20999038]\n",
      "[-0.05942671  0.13595513  0.1200097  -0.04403344]\n",
      "[-0.05670761  0.32917003  0.11912903 -0.29657301]\n",
      "[-0.05012421  0.5224101   0.11319757 -0.54943758]\n",
      "[-0.039676    0.71577521  0.10220882 -0.80441988]\n",
      "[-0.0253605   0.51941161  0.08612042 -0.48141455]\n",
      "[-0.01497227  0.32318634  0.07649213 -0.16287936]\n",
      "[-0.00850854  0.51713466  0.07323455 -0.43048439]\n",
      "[ 0.00183415  0.32105621  0.06462486 -0.11564337]\n",
      "[0.00825528 0.12507067 0.06231199 0.19670776]\n",
      "[ 0.01075669 -0.07088467  0.06624615  0.50837818]\n",
      "[ 0.009339   -0.26687437  0.07641371  0.82118001]\n",
      "[ 0.00400151 -0.46295411  0.09283731  1.13688512]\n",
      "[-0.00525757 -0.26916091  0.11557501  0.8747021 ]\n",
      "[-0.01064079 -0.07578384  0.13306905  0.62047508]\n",
      "[-0.01215647 -0.27248855  0.14547856  0.9519306 ]\n",
      "[-0.01760624 -0.07959212  0.16451717  0.70826239]\n",
      "[-0.01919808  0.11291509  0.17868242  0.47155071]\n",
      "[-0.01693978 -0.08422101  0.18811343  0.81480042]\n",
      "[-0.0186242  -0.28135203  0.20440944  1.16026025]\n",
      "Episode finished after 30 timesteps\n",
      "[0.01168989 0.00854987 0.00237378 0.0243497 ]\n",
      "[ 0.01186089  0.2036377   0.00286077 -0.26758333]\n",
      "[ 0.01593364  0.39871871 -0.0024909  -0.55936258]\n",
      "[ 0.02390802  0.59387553 -0.01367815 -0.85282923]\n",
      "[ 0.03578553  0.39894269 -0.03073473 -0.56447856]\n",
      "[ 0.04376438  0.59448208 -0.0420243  -0.86668371]\n",
      "[ 0.05565402  0.79014995 -0.05935798 -1.17227791]\n",
      "[ 0.07145702  0.98599122 -0.08280354 -1.48296344]\n",
      "[ 0.09117685  1.1820197  -0.11246281 -1.80031512]\n",
      "[ 0.11481724  0.9883207  -0.14846911 -1.54459651]\n",
      "[ 0.13458365  1.18488174 -0.17936104 -1.87968417]\n",
      "Episode finished after 11 timesteps\n",
      "[ 0.00701998 -0.00402898  0.00439984  0.04695401]\n",
      "[ 0.0069394  -0.19921374  0.00533892  0.34102187]\n",
      "[ 0.00295513 -0.39441125  0.01215936  0.63538359]\n",
      "[-0.0049331  -0.19946098  0.02486703  0.34655458]\n",
      "[-0.00892232 -0.39492766  0.03179812  0.6469741 ]\n",
      "[-0.01682087 -0.59047789  0.04473761  0.94949824]\n",
      "[-0.02863043 -0.39598581  0.06372757  0.67120049]\n",
      "[-0.03655014 -0.5919331   0.07715158  0.98324801]\n",
      "[-0.04838881 -0.78799921  0.09681654  1.29913239]\n",
      "[-0.06414879 -0.98420767  0.12279919  1.62048642]\n",
      "[-0.08383294 -0.79072779  0.15520892  1.36846434]\n",
      "[-0.0996475  -0.59785101  0.1825782   1.12807539]\n",
      "[-0.11160452 -0.40552819  0.20513971  0.89776636]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.04925818 -0.0008254   0.04537746 -0.03269241]\n",
      "[-0.04927469 -0.19656772  0.04472362  0.27395522]\n",
      "[-0.05320605 -0.39229832  0.05020272  0.5804019 ]\n",
      "[-0.06105201 -0.19791446  0.06181076  0.30394687]\n",
      "[-0.0650103  -0.39386034  0.0678897   0.61546519]\n",
      "[-0.07288751 -0.19974933  0.080199    0.34491366]\n",
      "[-0.0768825  -0.3959151   0.08709727  0.66176978]\n",
      "[-0.0848008  -0.59213404  0.10033267  0.98055675]\n",
      "[-0.09664348 -0.7884472   0.1199438   1.30299391]\n",
      "[-0.11241242 -0.9848689   0.14600368  1.63068861]\n",
      "[-0.1321098  -0.79173291  0.17861745  1.3868397 ]\n",
      "[-0.14794446 -0.5992297   0.20635425  1.15491365]\n",
      "Episode finished after 12 timesteps\n",
      "[0.00096378 0.04909881 0.01609046 0.00918678]\n",
      "[ 0.00194575 -0.14625016  0.0162742   0.3069027 ]\n",
      "[-0.00097925  0.04863616  0.02241225  0.01939634]\n",
      "[-6.52770275e-06  2.43429644e-01  2.28001765e-02 -2.66131857e-01]\n",
      "[0.00486207 0.04798983 0.01747754 0.03365435]\n",
      "[ 0.00582186 -0.14737834  0.01815063  0.33179997]\n",
      "[0.00287429 0.04748061 0.02478663 0.04489568]\n",
      "[ 0.00382391 -0.14798784  0.02568454  0.34529481]\n",
      "[ 0.00086415 -0.34346556  0.03259044  0.64596511]\n",
      "[-0.00600516 -0.53902611  0.04550974  0.94873006]\n",
      "[-0.01678568 -0.34454544  0.06448434  0.67068634]\n",
      "[-0.02367659 -0.15037645  0.07789807  0.39898345]\n",
      "[-0.02668412 -0.34651206  0.08587773  0.71517357]\n",
      "[-0.03361436 -0.15267722  0.10018121  0.45070982]\n",
      "[-0.03666791  0.04089573  0.1091954   0.19121028]\n",
      "[-0.03584999  0.23429989  0.11301961 -0.06512844]\n",
      "[-0.03116399  0.42763528  0.11171704 -0.32012425]\n",
      "[-0.02261129  0.62100364  0.10531455 -0.5755921 ]\n",
      "[-0.01019122  0.42457526  0.09380271 -0.25167672]\n",
      "[-0.00169971  0.22824774  0.08876918  0.06905748]\n",
      "[0.00286524 0.03197267 0.09015033 0.38837474]\n",
      "[ 0.0035047  -0.16430552  0.09791782  0.70806477]\n",
      "[2.18587496e-04 2.93335059e-02 1.12079118e-01 4.47739423e-01]\n",
      "[ 0.00080526 -0.16718065  0.12103391  0.77354349]\n",
      "[-0.00253836 -0.36374135  0.13650478  1.10172474]\n",
      "[-0.00981318 -0.17065334  0.15853927  0.85479485]\n",
      "[-0.01322625  0.02199404  0.17563517  0.61586333]\n",
      "[-0.01278637 -0.17509035  0.18795243  0.95831358]\n",
      "[-0.01628818  0.01707564  0.20711871  0.73007424]\n",
      "Episode finished after 29 timesteps\n",
      "[-0.04554279 -0.04550057 -0.00988621 -0.04421254]\n",
      "[-0.0464528  -0.24047937 -0.01077046  0.24533487]\n",
      "[-0.05126239 -0.04520525 -0.00586376 -0.05072577]\n",
      "[-0.0521665  -0.24024264 -0.00687828  0.24010134]\n",
      "[-0.05697135 -0.04502311 -0.00207625 -0.05474323]\n",
      "[-0.05787181 -0.24011523 -0.00317112  0.2372839 ]\n",
      "[-0.06267412 -0.04494811  0.00157456 -0.0563976 ]\n",
      "[-0.06357308  0.15015122  0.00044661 -0.34858332]\n",
      "[-0.06057005  0.34526682 -0.00652506 -0.64112538]\n",
      "[-0.05366472  0.54047912 -0.01934756 -0.93585596]\n",
      "[-0.04285514  0.34562338 -0.03806468 -0.64931488]\n",
      "[-0.03594267  0.54125432 -0.05105098 -0.95373704]\n",
      "[-0.02511758  0.346855   -0.07012572 -0.67752043]\n",
      "[-0.01818048  0.54287757 -0.08367613 -0.99143167]\n",
      "[-0.00732293  0.34896902 -0.10350476 -0.72615861]\n",
      "[-3.43549407e-04  5.45358254e-01 -1.18027937e-01 -1.04954199e+00]\n",
      "[ 0.01056362  0.74183134 -0.13901878 -1.37682096]\n",
      "[ 0.02540024  0.54869262 -0.1665552  -1.13064786]\n",
      "[ 0.03637409  0.74555661 -0.18916815 -1.47059859]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.04406581 -0.00638502 -0.0248273   0.03141728]\n",
      "[ 0.04393811  0.189084   -0.02419896 -0.26899438]\n",
      "[ 0.04771979 -0.0056844  -0.02957884  0.01595889]\n",
      "[ 0.0476061   0.18984899 -0.02925967 -0.2859078 ]\n",
      "[ 0.05140308  0.38537575 -0.03497782 -0.58767352]\n",
      "[ 0.0591106   0.19076065 -0.04673129 -0.30621082]\n",
      "[ 0.06292581 -0.00366531 -0.05285551 -0.02862426]\n",
      "[ 0.06285251 -0.197991   -0.05342799  0.24692492]\n",
      "[ 0.05889269 -0.00214831 -0.0484895  -0.06212029]\n",
      "[ 0.05884972 -0.19654271 -0.0497319   0.21487825]\n",
      "[ 0.05491887 -0.00074635 -0.04543434 -0.09306833]\n",
      "[ 0.05490394  0.19499637 -0.0472957  -0.39973251]\n",
      "[ 0.05880387  0.39075621 -0.05529035 -0.70694367]\n",
      "[ 0.06661899  0.58659879 -0.06942923 -1.01650571]\n",
      "[ 0.07835097  0.78257431 -0.08975934 -1.33015656]\n",
      "[ 0.09400245  0.9787066  -0.11636247 -1.64952447]\n",
      "[ 0.11357658  1.17498056 -0.14935296 -1.97607788]\n",
      "[ 0.1370762   0.98171507 -0.18887452 -1.73315363]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.04049378 -0.04411333  0.03464024  0.04801477]\n",
      "[-0.04137605 -0.23971443  0.03560053  0.3514226 ]\n",
      "[-0.04617033 -0.43532409  0.04262898  0.65511552]\n",
      "[-0.05487682 -0.24082076  0.05573129  0.37615471]\n",
      "[-0.05969323 -0.04653285  0.06325439  0.10155183]\n",
      "[-0.06062389 -0.24250161  0.06528542  0.41350149]\n",
      "[-0.06547392 -0.04836287  0.07355545  0.14209402]\n",
      "[-0.06644118  0.14563278  0.07639733 -0.12650679]\n",
      "[-0.06352852 -0.05049577  0.0738672   0.18926689]\n",
      "[-0.06453844 -0.24659251  0.07765254  0.50430709]\n",
      "[-0.06947029 -0.05264589  0.08773868  0.23707075]\n",
      "[-0.07052321  0.14112015  0.09248009 -0.02669822]\n",
      "[-0.0677008  -0.05519798  0.09194613  0.29367101]\n",
      "[-0.06880476 -0.25150234  0.09781955  0.61387783]\n",
      "[-0.07383481 -0.4478453   0.11009711  0.93569744]\n",
      "[-0.08279172 -0.64426612  0.12881105  1.260848  ]\n",
      "[-0.09567704 -0.45100524  0.15402801  1.01112416]\n",
      "[-0.10469714 -0.64780944  0.1742505   1.34794092]\n",
      "[-0.11765333 -0.45525238  0.20120932  1.11445004]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.00204229  0.01964729  0.00438592 -0.03556608]\n",
      "[ 0.00243524  0.21470607  0.00367459 -0.32686198]\n",
      "[ 0.00672936  0.40977552 -0.00286265 -0.61838384]\n",
      "[ 0.01492487  0.21469367 -0.01523032 -0.32660389]\n",
      "[ 0.01921874  0.01979183 -0.0217624  -0.03876258]\n",
      "[ 0.01961458  0.21521898 -0.02253765 -0.33823154]\n",
      "[ 0.02391896  0.41065427 -0.02930228 -0.63793553]\n",
      "[ 0.03213204  0.60617231 -0.04206099 -0.93970032]\n",
      "[ 0.04425549  0.4116418  -0.060855   -0.66052493]\n",
      "[ 0.05248833  0.60755544 -0.0740655  -0.97173173]\n",
      "[ 0.06463943  0.4135014  -0.09350013 -0.70320404]\n",
      "[ 0.07290946  0.60978624 -0.10756421 -1.02379427]\n",
      "[ 0.08510519  0.41624848 -0.1280401  -0.76672837]\n",
      "[ 0.09343016  0.22310001 -0.14337467 -0.51691833]\n",
      "[ 0.09789216  0.03025702 -0.15371303 -0.27262976]\n",
      "[ 0.0984973   0.22719997 -0.15916563 -0.60957494]\n",
      "[ 0.1030413   0.42414677 -0.17135713 -0.94785835]\n",
      "[ 0.11152423  0.62110989 -0.19031429 -1.28910883]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.04063321  0.03148657  0.01968197 -0.00496395]\n",
      "[ 0.04126295  0.2263208   0.0195827  -0.29137257]\n",
      "[0.04578936 0.03092518 0.01375524 0.00742161]\n",
      "[ 0.04640787  0.22584719  0.01390368 -0.28088983]\n",
      "[0.05092481 0.0305297  0.00828588 0.01614564]\n",
      "[ 0.0515354   0.22553185  0.00860879 -0.27391152]\n",
      "[0.05604604 0.03028813 0.00313056 0.02147416]\n",
      "[ 0.0566518   0.22536504  0.00356005 -0.2702194 ]\n",
      "[ 0.0611591   0.03019247 -0.00184434  0.02358426]\n",
      "[ 0.06176295  0.22534082 -0.00137266 -0.26968001]\n",
      "[ 0.06626977  0.03023849 -0.00676626  0.02256966]\n",
      "[ 0.06687454 -0.16478578 -0.00631486  0.3131101 ]\n",
      "[ 6.35788238e-02 -3.59817202e-01 -5.26624773e-05  6.03794843e-01]\n",
      "[ 0.05638248 -0.55493842  0.01202323  0.89646118]\n",
      "[ 0.04528371 -0.35998152  0.02995246  0.60758171]\n",
      "[ 0.03808408 -0.55550915  0.04210409  0.90954632]\n",
      "[ 0.0269739  -0.75117489  0.06029502  1.21515976]\n",
      "[ 0.0119504  -0.94702062  0.08459821  1.52611052]\n",
      "[-0.00699001 -0.75301551  0.11512042  1.2609858 ]\n",
      "[-0.02205032 -0.5595386   0.14034014  1.00646208]\n",
      "[-0.03324109 -0.36654109  0.16046938  0.7609367 ]\n",
      "[-0.04057192 -0.17395071  0.17568812  0.5227384 ]\n",
      "[-0.04405093  0.01831995  0.18614288  0.29015899]\n",
      "[-0.04368453  0.21036748  0.19194606  0.06147453]\n",
      "[-0.03947718  0.0130859   0.19317555  0.40804849]\n",
      "[-0.03921546  0.20501944  0.20133652  0.18194431]\n",
      "[-0.03511507  0.39677667  0.20497541 -0.04108751]\n",
      "[-0.02717954  0.58845988  0.20415366 -0.26275161]\n",
      "[-0.01541034  0.39109797  0.19889863  0.08674842]\n",
      "[-0.00758838  0.5828956   0.2006336  -0.13718463]\n",
      "[ 0.00406953  0.77466314  0.1978899  -0.36047446]\n",
      "[ 0.01956279  0.96650294  0.19068041 -0.58481748]\n",
      "[ 0.03889285  0.76929373  0.17898406 -0.23864419]\n",
      "[0.05427872 0.5721265  0.17421118 0.10472286]\n",
      "[ 0.06572125  0.76437947  0.17630564 -0.12833216]\n",
      "[ 0.08100884  0.95659517  0.173739   -0.36062047]\n",
      "[ 0.10014075  1.14887678  0.16652659 -0.59387811]\n",
      "[ 0.12311828  0.95186362  0.15464902 -0.2537136 ]\n",
      "[0.14215555 0.75491057 0.14957475 0.08347602]\n",
      "[0.15725377 0.55799632 0.15124427 0.41936062]\n",
      "[0.16841369 0.36109115 0.15963148 0.75564317]\n",
      "[0.17563552 0.55369491 0.17474435 0.51714328]\n",
      "[0.18670941 0.74598157 0.18508721 0.28421956]\n",
      "[0.20162904 0.54876863 0.1907716  0.62909192]\n",
      "[0.21260442 0.35156864 0.20335344 0.97527283]\n",
      "Episode finished after 45 timesteps\n",
      "[-0.04663245 -0.03167437  0.01261134  0.00850044]\n",
      "[-0.04726594  0.16326447  0.01278135 -0.28017693]\n",
      "[-0.04400065  0.35820179  0.00717781 -0.56880141]\n",
      "[-0.03683662  0.1629799  -0.00419822 -0.27386586]\n",
      "[-0.03357702 -0.0320819  -0.00967553  0.01748999]\n",
      "[-0.03421866  0.16317746 -0.00932573 -0.27822991]\n",
      "[-0.03095511  0.35843121 -0.01489033 -0.57383951]\n",
      "[-0.02378648  0.16352115 -0.02636712 -0.28588444]\n",
      "[-0.02051606  0.35900903 -0.03208481 -0.58676536]\n",
      "[-0.01333588  0.5545653  -0.04382012 -0.88938012]\n",
      "[-0.00224457  0.75025361 -0.06160772 -1.1955098 ]\n",
      "[ 0.0127605   0.9461167  -0.08551792 -1.50684827]\n",
      "[ 0.03168283  1.14216527 -0.11565488 -1.82495809]\n",
      "[ 0.05452614  1.338365   -0.15215404 -2.15121838]\n",
      "[ 0.08129344  1.53462134 -0.19517841 -2.48676272]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.04812526  0.02817498 -0.00549853 -0.04848357]\n",
      "[ 0.04868876  0.22337534 -0.0064682  -0.34289624]\n",
      "[ 0.05315627  0.41858871 -0.01332612 -0.63761181]\n",
      "[ 0.06152804  0.2236551  -0.02607836 -0.34915512]\n",
      "[ 0.06600114  0.41913806 -0.03306146 -0.64994602]\n",
      "[ 0.07438391  0.22449185 -0.04606038 -0.36785457]\n",
      "[ 0.07887374  0.03005362 -0.05341747 -0.09004352]\n",
      "[ 0.07947481  0.22589893 -0.05521834 -0.39908972]\n",
      "[ 0.08399279  0.42175895 -0.06320014 -0.70865793]\n",
      "[ 0.09242797  0.61769674 -0.0773733  -1.02054631]\n",
      "[ 0.10478191  0.42368629 -0.09778422 -0.75312635]\n",
      "[ 0.11325563  0.23003881 -0.11284675 -0.49274551]\n",
      "[ 0.11785641  0.42655644 -0.12270166 -0.81875361]\n",
      "[ 0.12638754  0.62312499 -0.13907673 -1.14737345]\n",
      "[ 0.13885004  0.43006541 -0.1620242  -0.90133712]\n",
      "[ 0.14745135  0.62696779 -0.18005094 -1.24024934]\n",
      "[ 0.1599907   0.82388567 -0.20485593 -1.58350069]\n",
      "Episode finished after 17 timesteps\n"
     ]
    }
   ],
   "source": [
    "#Visualitzem l'entorn\n",
    "for i_episode in range(15):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        #env.render()  #EL RENDER NOMÉS FUNCIONA EN LOCAL: comentar línia si no s'està en local.\n",
    "        print(observation)\n",
    "        action = env.action_space.sample() #acció aleatòria\n",
    "        observation, reward, done, info = env.step(action) #execució de l'acció triada\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1)) \n",
    "            break\n",
    "\n",
    "env.close() #tanquem la visualització de l'entorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recompensa és 1 per cada pas donat, inclòs l'estat terminal. Es considera l'entorn resolt quan la mitjana de les recompenses és major o igual a 195.0 després de 100 intents consecutius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcció d'una DQN: ensenyar un agent a jugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La construcció d'una DQN per ensenyar un agent a jugar a **CartPole** té, com hem vist en el mòdul didàctic, els passos següents:\n",
    "\n",
    "<ol>\n",
    "    <li> Definir el model de xarxa neuronal. </li>\n",
    "    <li> Definir l'agent: com s'ha de comportar, quan ha de seleccionar una acció i com. </li>\n",
    "    <li> Fixar hiperparàmetres. </li>\n",
    "    <li> Entrenar l'agent.  </li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Començarem important la llibreria per treballar en **Pytorch** i altres llibreries necessàries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Definició del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer pas és definir la nostra xarxa neuronal, la DQN. Per a aquest exemple, usarem una xarxa neuronal molt senzilla amb tres capes lineals i dues capes ReLU, a més de l'optimizador Adam.\n",
    "\n",
    "També indicarem la possibilitat de treballar amb **CPU** o **CUDA** per si es té l'opció, ja que en aprenentatge per reforç la majoria dels processos solen requerir molta màquina, i l'acceleració per maquinari és normalment necessària. **Aquest exemple es pot executar amb CPU**.\n",
    "\n",
    "\n",
    "Com s'explicava en el mòdul teòric, perquè l'aprenentatge prosperi és important que les aproximacions de _Q_ siguin prou bones perquè les experiències aportin informació rellevant a l'agent. Si no s'aconsegueixen bons valors, l'agent corre el risc d'estancar-se entre decisions dolentes sense mostrar cap millora. Per a això s'introdueix el **mètode <i>e-greedy</i>**, que permet a l'agent explorar accions aleatòries durant un temps a l'inici de l'entrenament i facilita que vagi passant a utilitzar l'aproximació de _Q_ a poc a poc (explotació). Recordem que aquest comportament ve definit per l'hiperparàmetre de probabilitat <i>epsilon</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GOMW16j-ZFDX"
   },
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ### Construcció de la xarxa neuronal\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, self.n_outputs, bias=True))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "        ### S'ofereix l'opció de treballar amb CUDA\n",
    "        if self.device == 'cuda':\n",
    "            self.model.cuda()\n",
    "            \n",
    "    \n",
    "    ### Mètode e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acció aleatòria\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acció a partir del càlcul del valor de Q per a aquesta acció\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. _Buffer_ de repetició d'experiències"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un altre problema de l'algorisme bàsic de la DQN era la seqüencialitat de les dades: els estats estan molt correlacionats i la xarxa neuronal no pot funcionar bé amb tanta correlació. Introduint un **<i>buffer</i> de repetició d'experiències**, permetem que s'emmagatzemin unes quantes experiències passades i que es passi un subconjunt aleatori d'aquestes experiències a la xarxa neuronal. Al seu torn, el *buffer* s'ha d'anar alimentant d'experiències noves conforme l'agent va aprenent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer importem les funcions `deque` i `namedtuple` de la llibreria `collections`. El *deque* és un objecte que emmagatzema valors fins a un límit fixat. Quan s'arriba al límit, el *deque* elimina el primer valor perquè pugui entrar-hi un de nou, i així successivament, de manera que es facilita, en el nostre cas, la retroalimentació del *buffer* amb experiències més noves i cada vegada més rellevants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim el __<i>buffer</i> de repetició d'experiències__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9F0QBdxTZKJl"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    ##Creem una llista d'índexs aleatoris i empaquetem les experiències en <i>arrays<i> de Numpy (això facilita el càlcul posterior de la pèrdua)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    ## S'afegeixen les noves experiències \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    ## Emplenem el <i>buffer<i> amb experiències aleatòries a l'inici de l'entrenament\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *burn-in* ens permet emplenar el *buffer* a l'inici de l'entrenament (quan l'agent encara no ha començat a explorar) amb experiències aleatòries perquè estigui prou ple per començar a entrenar amb una varietat d'informació bastant àmplia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Definició de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada tenim el model definit, només ens queda definir el comportament de l'agent, la manera com aprèn.\n",
    "\n",
    "Recordem que l'última millora que fèiem a la DQN bàsica i que ens permetia establir l'algorisme DQN final era la introducció d'una **xarxa objectiu**. Amb aquesta segona xarxa (còpia exacta de la principal), calculem el valor objectiu _Q'_, mentre que amb la xarxa principal calculem el valor de _Q_ actual. I cada cert temps se sincronitzen les dues xarxes. Així, aconseguim evitar que l'agent s'estanqui en una regió pel fet que la diferència entre estats (correlació) sigui tan petita que sempre triï la mateixa acció i que acabi per aprendre erròniament. \n",
    "\n",
    "Bàsicament, el procés que seguirà l'agent serà el següent:\n",
    "<ol>\n",
    "    <li> Emplenar el <i>buffer</i> amb unes quantes experiències aleatòries. </li>\n",
    "    <li> Interactuar amb l'entorn (fer un pas): \n",
    "        <ul>\n",
    "            <li> Prendre acció segons la probabilitat <i>epsilon</i>. </li>\n",
    "            <li> Emmagatzemar la informació en el <i>buffer</i>. </li>\n",
    "            <li> Obtenir la recompensa si està al final de l'episodi en qüestió. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Actualitzar la xarxa neuronal amb la freqüència que s'estableixi i calcular la pèrdua. </li>\n",
    "    <li> Sincronitzar la xarxa principal amb la xarxa objectiu amb la freqüència que s'estableixi. </li>\n",
    "    <li> Calcular la mitjana de les recompenses dels últims <i>X</i> episodis (generalment, 100). </li>\n",
    "    <li> Modificar el valor de <i>epsilon</i> per afavorir l'explotació enfront de l'exploració. </li>\n",
    "</ol>\n",
    "\n",
    "L'agent repetirà aquest procés fins que aconsegueixi l'objectiu a partir del qual es considera que ha après a jugar (en **CartPole** és 195, com s'indica en la variable `env.spec.reward_threshold`) o fins que s'esgoti el límit màxim d'episodis establert (hiperparàmetre fixat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BPcOyH4GY1mN"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork\n",
    "        self.target_network = deepcopy(dnnetwork) # xarxa objectiu (còpia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = 100 # bloc dels X últims episodis dels quals es calcularà la mitjana de recompensa\n",
    "        self.reward_threshold = self.env.spec.reward_threshold # recompensa mitjana a partir de la qual es considera\n",
    "                                                               # que l'agent ha après a jugar\n",
    "        self.initialize()\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()\n",
    "        \n",
    "    \n",
    "    ## Prenem una nova acció\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore': \n",
    "            # acció aleatòria en el burn-in i en la fase d'exploració (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # acció a partir del valor de Q (elecció de l'acció amb millor Q)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Fem l'acció i obtenim el nou estat i la recompensa\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardem experiència en el buffer\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = env.reset()\n",
    "        return done\n",
    "\n",
    "    \n",
    "        \n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=50000, \n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Emplenem el buffer amb N experiències aleatòries ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # L'agent pren una acció\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "               \n",
    "                # Actualitzem la xarxa principal segons la freqüència establerta\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sincronitzem la xarxa principal i la xarxa objectiu segons la freqüència establerta\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                \n",
    "                if gamedone:                   \n",
    "                    episode += 1\n",
    "                    self.training_rewards.append(self.total_reward) # guardem les recompenses obtingudes\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(   # calculem la mitjana de recompensa dels últims X episodis\n",
    "                        self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "                    \n",
    "                    # Comprovem que encara queden episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    # Acaba el joc si la mitjana de recompenses ha arribat al llindar fixat per a aquest joc  \n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "                    \n",
    "                    # Actualitzem epsilon segons la velocitat de decaïment fixada\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "                    \n",
    "                \n",
    "    ## Càlcul de la pèrdua                   \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separem les variables de l'experiència i les convertim en tensors \n",
    "        states, actions, rewards, dones, next_states = [i for i in batch] \n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.dnnetwork.device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.dnnetwork.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.dnnetwork.device)\n",
    "        \n",
    "        # Obtenim els valors de Q de la xarxa principal\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenim els valors de Q objectiu. El paràmetre detach() evita que aquests valors actualitzin la xarxa objectiu\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # 0 en estats terminals\n",
    "        \n",
    "        # Calculem l'equació de Bellman\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "        \n",
    "        # Calculem la pèrdua\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    \n",
    "    def update(self):\n",
    "        self.dnnetwork.optimizer.zero_grad()  # eliminem qualsevol gradient passat\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionem un conjunt del <i>buffer<i>\n",
    "        loss = self.calculate_loss(batch) # calculem la pèrdua\n",
    "        loss.backward() # fem la diferència per obtenir els gradients\n",
    "        self.dnnetwork.optimizer.step() # apliquem els gradients a la xarxa neuronal\n",
    "        # Guardem els valors de pèrdua\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "            \n",
    "\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(self.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hiperparàmetres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixem els hiperparàmetres necessaris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2625S-lDWxag"
   },
   "outputs": [],
   "source": [
    "lr = 0.001            #Velocitat d'aprenentatge\n",
    "MEMORY_SIZE = 100000  #Màxima capacitat del buffer\n",
    "MAX_EPISODES = 5000   #Nombre màxim d'episodis (l'agent ha d'aprendre abans d'arribar a aquest valor)\n",
    "EPSILON = 1           #Valor inicial d'epsilon\n",
    "EPSILON_DECAY = .99   #Decaïment d'epsilon\n",
    "GAMMA = 0.99          #Valor gamma de l'equació de Bellman\n",
    "BATCH_SIZE = 32       #Conjunt a agafar del buffer per a la xarxa neuronal\n",
    "BURN_IN = 1000        #Nombre d'episodis inicials usats per emplenar el buffer abans d'entrenar\n",
    "DNN_UPD = 1           #Freqüència d'actualització de la xarxa neuronal \n",
    "DNN_SYNC = 2500       #Freqüència de sincronització de pesos entre la xarxa neuronal i la xarxa objectiu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Entrenament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el *buffer* de repetició d'experiències:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model de xarxa neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el nostre agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env, dqn, buffer, EPSILON, EPSILON_DECAY, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem l'agent amb els hiperparàmetres establerts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agusIfEMZlKi",
    "outputId": "47b8253f-5857-4b63-d60c-442b983d1d3c"
   },
   "outputs": [],
   "source": [
    "agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, \n",
    "              batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Representació de l'aprenentatge de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "9ghHg3rH3MTV",
    "outputId": "a7c5d4cb-5e98-45ac-ca70-8dcfdffaa86a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.plot_rewards()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OriginalBlog_mod.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
